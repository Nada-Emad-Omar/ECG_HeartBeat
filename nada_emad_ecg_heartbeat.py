# -*- coding: utf-8 -*-
"""Nada_Emad_ECG_HeartBeat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xMSItuNPZEXpFOVOIJWQkYaLe7uAdz0u
"""

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d shayanfazeli/heartbeat

!unzip /content/heartbeat.zip

import pandas as pd
data = pd.read_csv('/content/mitbih_train.csv')

print(data.shape)

# Create a new list of column names
new_column_names = ['col_' + str(i) for i in range(188)]

# Rename the columns
data.columns = new_column_names

data.describe()

data.info()

data.isnull().sum()

#last column contains the class labels
X = data.iloc[:, :-1]  # Features
y = data.iloc[:, -1]   # Labels

# Now you have X (features) and y (labels) for your classification task.

from imblearn.over_sampling import SMOTE

# Apply SMOTE to balance the data
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Now X_resampled and y_resampled contain the balanced dataset

import pandas as pd
print("Shape of resampled data:", X_resampled.shape)
print("Class distribution after SMOTE:")
print(pd.Series(y_resampled).value_counts())

import matplotlib.pyplot as plt
# Select the first 5 rows
first_5_rows = data.iloc[:5, :-1]  # Exclude the last column (labels)

# Create subplots
fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(10, 20))

# Iterate over the rows and plot each column
for i in range(5):
  row = first_5_rows.iloc[i]
  axes[i].plot(row)
  axes[i].set_title(f"Row {i+1}")
  axes[i].set_xlabel("Column Index")
  axes[i].set_ylabel("Value")

plt.tight_layout()
plt.show()

# Count the number of zero values in each row
zero_counts = (data == 0).sum(axis=1)

# Filter out rows with more than 140 zero values
data = data[zero_counts <= 140]

# Print the shape of the filtered data
print("Shape of the filtered data:", data.shape)

# Assuming the last column contains the class labels
X = data.iloc[:, :-1]  # Features
y = data.iloc[:, -1]   # Labels

# Filter data for classes 1 and 3
data_filtered = data[data.iloc[:, -1].isin([1, 3])]

# Separate features and labels for the filtered data
X_filtered = data_filtered.iloc[:, :-1]
y_filtered = data_filtered.iloc[:, -1]

# Calculate the IQR for each feature
Q1 = X_filtered.quantile(0.25)
Q3 = X_filtered.quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = ((X_filtered < lower_bound) | (X_filtered > upper_bound)).any(axis=1)

# Remove outliers
data_filtered = data_filtered[~outliers]

# Separate features and labels after removing outliers
X_filtered = data_filtered.iloc[:, :-1]
y_filtered = data_filtered.iloc[:, -1]

print(data.shape)

print(X.shape)

import pandas as pd
print("Shape of resampled data:", X_resampled.shape)
print("Class distribution after SMOTE:")
print(pd.Series(y_resampled).value_counts())

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import time
import seaborn as sns

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=0.95)  # Keep components that explain 95% of the variance
X_pca = pca.fit_transform(X_scaled)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Create a Random Forest classifier with adjusted parameters for recall
rf_classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)

# Start the timer
start_time = time.time()

# Train the model
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# End the timer
end_time = time.time()

# Calculate the time taken
time_taken = end_time - start_time

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Generate the classification report
report = classification_report(y_test, y_pred)
print(report)

# Create a confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=data.iloc[:, -1].unique(),
            yticklabels=data.iloc[:, -1].unique())
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Print the time taken
print(f"Time taken for model training and prediction: {time_taken:.4f} seconds")

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
# Define the model
model = keras.Sequential([
    layers.Input(shape=(X_train.shape[1],)),  # Input layer with the number of features
    layers.Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation
    layers.Dense(64, activation='relu'),   # Another hidden layer
    layers.Dense(5, activation='softmax')  # Output layer with 5 neurons (for 5 classes) and softmax activation
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
start_time = time.time()
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
end_time = time.time()
time_taken = end_time - start_time

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print('Test accuracy:', accuracy)

# Print the time taken
print(f"Time taken for model training and prediction: {time_taken:.4f} seconds")

# Plot training and validation accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training and validation loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print('Test accuracy:', accuracy)

# Generate the classification report
y_pred_probs = model.predict(X_test)
y_pred = [tf.math.argmax(pred).numpy() for pred in y_pred_probs]
report = classification_report(y_test, y_pred)
print(report)

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
# Load the test data
test_data = pd.read_csv('/content/mitbih_test.csv')

# Create a new list of column names
new_column_names = ['col_' + str(i) for i in range(188)]

# Rename the columns
test_data.columns = new_column_names

# Separate features and labels
X_test_new = test_data.iloc[:, :-1]  # Features
y_test_new = test_data.iloc[:, -1]   # Labels

# Standardize the test data
X_test_scaled = scaler.transform(X_test_new)

# Apply PCA to the test data
X_test_pca = pca.transform(X_test_scaled)

# Make predictions on the new test data using the trained model
y_pred_new = model.predict(X_test_pca)
y_pred_new = [np.argmax(pred) for pred in y_pred_new]

# Evaluate the model on the new test data
accuracy_new = accuracy_score(y_test_new, y_pred_new)
print("Accuracy on new test data:", accuracy_new)

# Generate the classification report for the new test data
report_new = classification_report(y_test_new, y_pred_new)
print(report_new)

# Create a confusion matrix for the new test data
cm_new = confusion_matrix(y_test_new, y_pred_new)

# Plot the confusion matrix for the new test data
plt.figure(figsize=(8, 6))
sns.heatmap(cm_new, annot=True, fmt="d", cmap="Blues",
            xticklabels=test_data.iloc[:, -1].unique(),
            yticklabels=test_data.iloc[:, -1].unique())
plt.title("Confusion Matrix (New Test Data)")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# prompt: model using simple cnn

import matplotlib.pyplot as plt
# Reshape the data for CNN input
X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Define the CNN model
model_cnn = keras.Sequential([
    layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),
    layers.MaxPooling1D(pool_size=2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(5, activation='softmax')
])

# Compile the model
model_cnn.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

# Train the model
start_time = time.time()
history_cnn = model_cnn.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2)
end_time = time.time()
time_taken_cnn = end_time - start_time

# Evaluate the model
loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test_cnn, y_test, verbose=0)
print('Test accuracy (CNN):', accuracy_cnn)

# Print the time taken
print(f"Time taken for CNN model training and prediction: {time_taken_cnn:.4f} seconds")

# Plot training and validation accuracy for CNN
plt.plot(history_cnn.history['accuracy'])
plt.plot(history_cnn.history['val_accuracy'])
plt.title('CNN Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training and validation loss for CNN
plt.plot(history_cnn.history['loss'])
plt.plot(history_cnn.history['val_loss'])
plt.title('CNN Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Generate the classification report for CNN
y_pred_probs_cnn = model_cnn.predict(X_test_cnn)
y_pred_cnn = [tf.math.argmax(pred).numpy() for pred in y_pred_probs_cnn]
report_cnn = classification_report(y_test, y_pred_cnn)
print(report_cnn)